{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eedcfa2-5c8d-4875-9f74-f206c032dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def update_master(taxi_trips: pd.DataFrame, master: pd.DataFrame, id_column: str, value_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extend master DataFrame with new values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    taxi_trips : pd.DataFrame\n",
    "        A DataFrame containing taxi trip records, with a column \"company\" indicating the company \n",
    "        associated with each trip.\n",
    "\n",
    "    master : pd.DataFrame\n",
    "        A DataFrame containing the master data.\n",
    "\n",
    "    id_column: str\n",
    "        The id column of the master data.\n",
    "\n",
    "    value_column: str\n",
    "        The value column of the master data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the updated company master list, including any new companies found \n",
    "        in the `taxi_trips` DataFrame that were not already present in `company_master`.\n",
    "    \"\"\"\n",
    "    max_id = master[id_column].max()\n",
    "    #new_values_list = [values for values in taxi_trips[value_column].values if values not in master[value_colomn].values]\n",
    "    new_values_list = list(set(taxi_trips[value_column].values)-set(master[value_column].values))\n",
    "    new_values_df = pd.DataFrame({\n",
    "        id_column: range(max_id +1, max_id + len(new_values_list)+1),\n",
    "        value_column: new_values_list\n",
    "    })\n",
    "    updated_master = pd.concat([master, new_values_df], ignore_index=True)\n",
    "    return updated_master\n",
    "\n",
    "def update_taxi_trips_with_master_data(taxi_trips: pd.DataFrame, payment_type_master: pd.DataFrame, company_master:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Update the taxi trips data by merging it with payment type and company master data.\n",
    "\n",
    "    This function enriches the `taxi_trips` DataFrame by merging it with the `payment_type_master` \n",
    "    and `company_master` DataFrames. It adds relevant information from these master data tables \n",
    "    based on matching `payment_type` and `company` columns. After the merges, it drops the original \n",
    "    `payment_type` and `company` columns from the `taxi_trips` DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    taxi_trips : pd.DataFrame\n",
    "        A DataFrame containing the taxi trip records, with at least two columns:\n",
    "        - \"payment_type\": The payment method used for each trip.\n",
    "        - \"company\": The company associated with each trip.\n",
    "\n",
    "    payment_type_master : pd.DataFrame\n",
    "        A DataFrame containing the payment type details. It must have at least the following column:\n",
    "        - \"payment_type\": The payment method identifier.\n",
    "    \n",
    "    company_master : pd.DataFrame\n",
    "        A DataFrame containing company details. It must have at least the following column:\n",
    "        - \"company\": The company identifier or name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the original taxi trip records enriched with additional columns from\n",
    "        the `payment_type_master` and `company_master`. The original `payment_type` and `company` columns\n",
    "        are dropped.\n",
    "    \"\"\"\n",
    "    taxi_trips_id=taxi_trips.merge(payment_type_master, on=\"payment_type\")\n",
    "    taxi_trips_id=taxi_trips_id.merge(company_master, on=\"company\")\n",
    "    taxi_trips_id.drop([\"payment_type\", \"company\"], axis=1, inplace=True)\n",
    "    return taxi_trips_id\n",
    "\n",
    "def taxi_trips_transformations(taxi_trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform transformations with the taxi data.\n",
    "\n",
    "    Parameters\n",
    "    taxi_trips: pd.DataFrame\n",
    "        The DataFrame holding the daily taxi trips.\n",
    "\n",
    "    Returns\n",
    "    pd.DataFrame\n",
    "        The clean transformed DataFrame holding the daily taxi trips.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(taxi_trips, pd.DataFrame):\n",
    "        raise TypeError(\"'taxi_trips' is not a valid pandas DataFrame.\")\n",
    "\n",
    "    taxi_trips.drop([\"pickup_census_tract\", \"dropoff_census_tract\",\"pickup_centroid_location\", \"dropoff_centroid_location\"], axis=1, inplace=True)\n",
    "    taxi_trips.dropna(inplace=True)\n",
    "    taxi_trips.rename(columns={\"pickup_community_area\" : \"pickup_community_area_id\", \"dropoff_community_area\" : \"dropoff_community_area_id\"}, inplace=True)\n",
    "    taxi_trips[\"datetime_for_weather\"]= pd.to_datetime(taxi_trips[\"trip_start_timestamp\"]).dt.floor(\"h\")\n",
    "    return taxi_trips\n",
    "\n",
    "def transform_weather_data(weather_data: json) -> pd.DataFrame:    \n",
    "    \"\"\"r\n",
    "    Transform raw weather data from a JSON format into a structured DataFrame.\n",
    "\n",
    "    This function extracts relevant weather data from the given JSON object and transforms it into\n",
    "    a pandas DataFrame. It filters out the necessary fields such as \"datetime\", \"temperature\", \"wind_speed\",\n",
    "    \"rain\", and \"precipitation\" from the hourly weather data, then converts the \"datetime\" field to a pandas \n",
    "    datetime format for easier handling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    weather_data : json\n",
    "        A JSON object containing weather data. The expected structure is a dictionary with the following\n",
    "        keys and nested fields:\n",
    "        - \"hourly\": A dictionary containing the hourly weather data.\n",
    "            - \"time\": A list of datetime values for each hour.\n",
    "            - \"temperature_2m\": A list of temperature values at 2 meters above ground.\n",
    "            - \"windspeed_10m\": A list of wind speed values at 10 meters above ground.\n",
    "            - \"rain\": A list of rainfall amounts for each hour.\n",
    "            - \"precipitation\": A list of total precipitation amounts for each hour.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the following columns:\n",
    "        - \"datetime\": The timestamp for each hour.\n",
    "        - \"temperature\": Temperature values at 2 meters above ground.\n",
    "        - \"wind_speed\": Wind speed values at 10 meters above ground.\n",
    "        - \"rain\": Rainfall amounts for each hour.\n",
    "        - \"precipitation\": Total precipitation amounts for each hour.\n",
    "    \"\"\"\n",
    "    weather_data_filtered = {\n",
    "        \"datetime\" : weather_data[\"hourly\"][\"time\"],\n",
    "        \"temperature\" : weather_data[\"hourly\"][\"temperature_2m\"],\n",
    "        \"wind_speed\" : weather_data[\"hourly\"][\"windspeed_10m\"],\n",
    "        \"rain\" : weather_data[\"hourly\"][\"rain\"],\n",
    "        \"precipitation\" : weather_data[\"hourly\"][\"precipitation\"],\n",
    "\n",
    "    }\n",
    "    weather_df = pd.DataFrame(weather_data_filtered)\n",
    "    weather_df[\"datetime\"]= pd.to_datetime(weather_df.datetime)\n",
    "    return weather_df\n",
    "\n",
    "def read_csv_from_s3(bucket: str, path: str, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file from an S3 bucket and loads it into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket.\n",
    "        key (str): The S3 key (folder path) where the file is located.\n",
    "        filename (str): The name of the CSV file to be read.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing the data from the CSV file.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the CSV file is UTF-8 encoded.\n",
    "    \"\"\"  \n",
    "    s3 = boto3.client('s3')\n",
    "    full_path = f\"{path}{filename}\"\n",
    "    object = s3.get_object(Bucket=bucket, Key=full_path)\n",
    "    object = object['Body'].read().decode('utf-8')\n",
    "    output_df = pd.read_csv(StringIO(object))\n",
    "    return output_df\n",
    "\n",
    "def read_json_from_s3(bucket: str, key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads a JSON file from an S3 bucket and loads it into a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket where the JSON file is stored.\n",
    "        key (str): The S3 key (path) to the JSON file within the bucket.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON content from the file as a Python dictionary.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body']\n",
    "    json_data = json.loads(content.read())\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def upload_dataframe_to_s3(dataframe: pd.DataFrame, bucket: str, path: str):\n",
    "    \"\"\"\n",
    "    Uploads a Pandas DataFrame to an S3 bucket as a CSV file.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame to upload.\n",
    "        bucket (str): The name of the S3 bucket where the file will be stored.\n",
    "        path (str): The S3 object key (file path) where the file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything. It uploads the DataFrame to S3.\n",
    "\n",
    "    Example:\n",
    "        upload_dataframe_to_s3(df, 'my-bucket', 'path/to/myfile.csv')\n",
    "\n",
    "    Notes:\n",
    "        Ensure that AWS credentials are configured and the necessary permissions are granted to access the S3 bucket.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    buffer = StringIO()\n",
    "    dataframe.to_csv(buffer, index=False)\n",
    "    df_content = buffer.getvalue()\n",
    "    s3.put_object(Bucket = bucket, Key = path, Body=df_content)\n",
    "\n",
    "\n",
    "def upload_master_data_to_s3(bucket: str, path: str, file_type: str, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Uploads the given DataFrame as a CSV file to an S3 bucket, and creates a backup of the previous version.\n",
    "\n",
    "    This function first copies the existing master file in S3 to create a backup with a 'previous_version' suffix.\n",
    "    Then, it uploads the new DataFrame to S3, overwriting the master file.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket where the file will be uploaded.\n",
    "        path (str): The S3 folder path where the master file is located.\n",
    "        file_type (str): The type of file, used to construct the filename for the master CSV file.\n",
    "        DataFrame (pd.DataFrame): The DataFrame to be uploaded as a CSV to the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    master_file_path = f\"{path}{file_type}_master.csv\"\n",
    "    previous_master_file_path = \"/transformed_data/master_table_previous_version/{file_type}_master_previous_version.csv\"\n",
    "\n",
    "    s3.copy_object(Bucket=bucket, CopySource={'Bucket': bucket, 'Key': master_file_path}, Key=previous_master_file_path)\n",
    "    upload_dataframe_to_s3(dataframe, bucket, master_file_path)\n",
    "\n",
    "def upload_and_move_file_on_s3(dataframe: pd.DataFrame, datetime_col: str, bucket: str,file_type: str, filename: str, source_path: str, target_path_raw: str, target_path_transformed: str):\n",
    "    \"\"\"\n",
    "    Uploads a Pandas DataFrame to an S3 bucket, moves the file to a raw location, \n",
    "    and deletes the original file from the source path.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Formats the date from the specified `datetime_col` of the DataFrame.\n",
    "    2. Uploads the DataFrame to an S3 path based on the transformed date.\n",
    "    3. Copies the uploaded file to a \"raw\" location within the same S3 bucket.\n",
    "    4. Deletes the original file from the source path in the S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame to be uploaded.\n",
    "        datetime_col (str): The column name in the DataFrame containing the datetime values used for formatting the file name.\n",
    "        bucket (str): The name of the S3 bucket where the file will be uploaded.\n",
    "        file_type (str): A string used in the file name to denote the file type (e.g., \"raw\", \"transformed\").\n",
    "        filename (str): The name of the file to be copied and deleted within S3.\n",
    "        source_path (str): The S3 path where the original file is located before it is copied and deleted.\n",
    "        target_path_raw (str): The S3 path where the file will be copied to as a \"raw\" file.\n",
    "        target_path_transformed (str): The S3 path where the file will be uploaded as a transformed file.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value. It performs the file upload, copy, and deletion on S3.\n",
    "\n",
    "    Example:\n",
    "        upload_and_move_file_on_s3(df, 'date_column', 'my-bucket', 'data', 'data.csv', 'source/path/', 'raw/path/', 'transformed/path/')\n",
    "\n",
    "    Notes:\n",
    "        Ensure that AWS credentials are configured and that the necessary permissions are granted to access and modify the S3 bucket.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    formatted_date = dataframe[datetime_col].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    new_path_with_filename = f\"{target_path_transformed}{file_type}_{formatted_date}.csv\"\n",
    "\n",
    "    upload_dataframe_to_s3(dataframe, bucket, new_path_with_filename)\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket, \n",
    "        CopySource={'Bucket': bucket, 'Key': f\"{source_path}{filename}\"}, \n",
    "        Key=f\"{target_path_raw}{filename}\")\n",
    "    \n",
    "    s3.delete_object(Bucket=bucket, Key=f\"{source_path}{filename}\")\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = 'cubix-chicago-taxi-sp'\n",
    "    raw_weather_folder = 'raw_data/to_processed/weather_data/'\n",
    "    raw_taxi_trips_folder = 'raw_data/to_processed/taxi_data/'\n",
    "\n",
    "    target_taxi_trips_folder = 'raw_data/processed/taxi_data/'\n",
    "    target_weather_folder = 'raw_data/processed/weather_data/'\n",
    "\n",
    "    transformed_taxi_trips_folder = 'transformed_data/taxi_data/'\n",
    "    transformed_weather_folder = 'transformed_data/weather/'\n",
    "\n",
    "    payment_type_master_folder = 'transformed_data/payment_type/'\n",
    "    company_master_folder = 'transformed_data/company/'\n",
    "\n",
    "    payment_type_master_file_name = 'payment_type_master.csv'\n",
    "    company_master_file_name = 'company_master.csv'\n",
    "\n",
    "    payment_type_master = read_csv_from_s3(bucket, payment_type_master_folder, payment_type_master_file_name)\n",
    "    company_master = read_csv_from_s3(bucket, company_master_folder, company_master_file_name)\n",
    "\n",
    "# Taxi data transformation and loading   \n",
    "    for file in s3.list_objects(Bucket=bucket, Prefix = raw_taxi_trips_folder)['Contents']:\n",
    "        taxi_trips_key = file['Key']\n",
    "\n",
    "        if taxi_trips_key.split('/')[-1].strip() != '':\n",
    "            if taxi_trips_key.split('.')[1] == 'json':\n",
    "                filename = taxi_trips_key.split('/')[-1]\n",
    "\n",
    "                taxi_trips_data_json = read_json_from_s3(bucket=bucket, key=taxi_trips_key)\n",
    "\n",
    "                taxi_trips_data_raw = pd.DataFrame(taxi_trips_data_json)\n",
    "                taxi_trips_transformed = taxi_trips_transformations(taxi_trips_data_raw)\n",
    "\n",
    "                company_master_updated = update_master(taxi_trips_transformed, company_master, \"company_id\", \"company\")\n",
    "                payment_type_master_updated = update_master(taxi_trips_transformed, payment_type_master, \"payment_type_id\", \"payment_type\")\n",
    "\n",
    "                taxi_trips = update_taxi_trips_with_master_data(taxi_trips_transformed, payment_type_master_updated, company_master_updated)\n",
    "\n",
    "                upload_and_move_file_on_s3(dataframe= taxi_trips, datetime_col = \"datetime_for_weather\", bucket= bucket, file_type= \"taxi\", filename = filename, source_path = raw_taxi_trips_folder, target_path_raw = target_taxi_trips_folder, target_path_transformed = transformed_taxi_trips_folder)\n",
    "\n",
    "\n",
    "                upload_master_data_to_s3(bucket, company_master_folder, \"company\", company_master_updated)\n",
    "                upload_master_data_to_s3(bucket, payment_type_master_folder, \"payment_type\", payment_type_master_updated)\n",
    "                print(f\"company_master_updated: {company_master_updated}\")\n",
    "                print(f\"payment_type_master_updated: {payment_type_master_updated}\")\n",
    "\n",
    "# Weather data transformation and loading\n",
    "    for file in s3.list_objects(Bucket=bucket, Prefix = raw_weather_folder)['Contents']:\n",
    "        weather_key = file['Key']\n",
    "\n",
    "        if weather_key.split('/')[-1].strip() != '':\n",
    "            if weather_key.split('.')[1] == 'json':\n",
    "\n",
    "                filename = weather_key.split('/')[-1]\n",
    "\n",
    "                weather_data_json = read_json_from_s3(bucket=bucket, key=weather_key)\n",
    "\n",
    "                weather_data = transform_weather_data(weather_data_json)\n",
    "\n",
    "                upload_and_move_file_on_s3(dataframe= weather_data, datetime_col= \"datetime\", bucket= bucket, file_type= \"weather\", filename = filename, source_path = raw_weather_folder, target_path_raw = target_weather_folder, target_path_transformed= transformed_weather_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
